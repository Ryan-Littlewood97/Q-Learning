# -*- coding: utf-8 -*-
"""10170832_Assignment9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1THdZlYGItr4JqjH8EVlD4jsCCPt4AN4C
"""

#pip install gym

import gym
import numpy as np
import random
from IPython.display import clear_output

env = gym.make("FrozenLake-v0").env
env.render()

env.reset() # reset environment to a new, random state
env.render()

# print numbers of actions and states
print("Action Space {}".format(env.action_space))
print("State Space {}".format(env.observation_space))
#
q_table = np.zeros([env.observation_space.n, env.action_space.n])
print(q_table)

''' Using Q-Learning'''
'''THis is the training portion of the code, to train the model against the game'''

# Hyperparameters
alpha = 0.1
gamma = 0.6
epsilon = 1  #This is the starting epsilon value. 1 = Explore, 0 = Exploit to the full extent of the probelm 
decay_rate = 1.0/100000   #This is the rate of decay for epsilon, to transition from explore to exploit 

for i in range(1,100001):
    state = env.reset()

    epochs, penalties, reward, = 0, 0, 0
    done = False
    
    while not done:
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample() 
        else:
            action = np.argmax(q_table[state]) 

        next_state, reward, done, info = env.step(action) 
        
        old_value = q_table[state, action]
        next_max = np.max(q_table[next_state])
        
        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
        q_table[state, action] = new_value

        if reward == 1:
            penalties += 0

        state = next_state
        epochs += 1

        if epsilon > 0: #This is the way we can transition epsilon from 1 to 0 using the rate of decay against the original value 
          epsilon -= decay_rate

    if i % 100 == 0:
        clear_output(wait=True)
        print(f"Episode: {i}")


print("Training finished on 100000 episodes.\n")

'''This is the testing portion of the code, to see how well our now trained agent does within the game'''

total_epochs, total_penalties, total_reward = 0, 0, 0
episodes = 100000

for _ in range(episodes):
    state = env.reset()
    epochs, penalties, reward = 0, 0, 0
    
    done = False
    
    while not done:
        action = np.argmax(q_table[state])
        state, reward, done, info = env.step(action)

        if reward == 1:
            penalties += 0

        epochs += 1

    total_penalties += penalties #Will always be 0, as penalties are 0 in this problem
    total_epochs += epochs
    total_reward += reward #By using 1- the reward value, we can get the total amount of failures, so there is no real need to store the failures as 1= success so 1- = failure
    
'''These are the printing metrics for our problem. Successes, failures, average time step, and the full Q-Table'''

print(f"Results after {episodes} episodes:")
print(f"Average timesteps per episode: {total_epochs / episodes}")
print("Total reward: {}".format(total_reward))
print(f"Total Failures: {episodes - total_reward}")
print(q_table)